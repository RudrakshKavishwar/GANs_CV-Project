# -*- coding: utf-8 -*-
"""GANs Project_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxnjQoEwgLwj4k5lQuxkVZg16P6Tqxzj
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
import os

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_path = "/content/drive/MyDrive/DisasterModel.zip"  # Update this with the actual zip path
extract_path = "/content/disaster_dataset"  # Define the extraction directory

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Dataset extracted successfully!")

# Image dimensions
IMG_WIDTH = 64
IMG_HEIGHT = 64
BATCH_SIZE = 32

# Image data generator for training and testing
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

dataset_path = "/content/disaster_dataset"
# Load Training Data
train_generator = train_datagen.flow_from_directory(
    os.path.join(dataset_path, "train"),
    target_size=(IMG_WIDTH, IMG_HEIGHT),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset="training"
)

# Load Validation Data
val_generator = train_datagen.flow_from_directory(
    os.path.join(dataset_path, "train"),
    target_size=(IMG_WIDTH, IMG_HEIGHT),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset="validation"
)

# Define CNN Model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(len(train_generator.class_indices), activation='softmax') # Output layer
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
EPOCHS = 20
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=val_generator
)

# Plot Accuracy and Loss Curves
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title("Model Accuracy")

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title("Model Loss")

plt.show()

# Load test images
test_generator = train_datagen.flow_from_directory(
    os.path.join(dataset_path, "test"),
    target_size=(IMG_WIDTH, IMG_HEIGHT),
    batch_size=1,
    class_mode='categorical',
    shuffle=False
)

# Predict on test images
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)

# Display a few test images with predictions
fig, axes = plt.subplots(3, 3, figsize=(8, 8))
for i, ax in enumerate(axes.flat):
    img = test_generator[i][0][0]
    ax.imshow(img)
    ax.set_title(f"Predicted: {list(train_generator.class_indices.keys())[predicted_classes[i]]}")
    ax.axis("off")

plt.show()

import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tqdm import tqdm

# Define dataset path (after extraction)
dataset_path = "/content/disaster_dataset/train"  # Adjust if needed

IMG_WIDTH = 64
IMG_HEIGHT = 64
CHANNELS = 3
BATCH_SIZE = 32

# Image Data Generator
datagen = ImageDataGenerator(rescale=1.0/255.0)
train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(IMG_WIDTH, IMG_HEIGHT),
    batch_size=BATCH_SIZE,
    class_mode=None,  # No labels for GANs
    shuffle=True
)

def build_generator():
    model = Sequential([
        Dense(8*8*256, activation="relu", input_shape=(100,)),
        Reshape((8, 8, 256)),

        Conv2DTranspose(128, (4,4), strides=(2,2), padding="same", activation="relu"),
        BatchNormalization(),

        Conv2DTranspose(64, (4,4), strides=(2,2), padding="same", activation="relu"),
        BatchNormalization(),

        Conv2DTranspose(3, (4,4), strides=(2,2), padding="same", activation="tanh")
    ])
    return model

generator = build_generator()
generator.summary()

def build_discriminator():
    model = Sequential([
        Conv2D(64, (4,4), strides=(2,2), padding="same", input_shape=(64,64,3)),
        LeakyReLU(alpha=0.2),
        Dropout(0.3),

        Conv2D(128, (4,4), strides=(2,2), padding="same"),
        LeakyReLU(alpha=0.2),
        Dropout(0.3),

        Flatten(),
        Dense(1, activation="sigmoid")
    ])
    return model

discriminator = build_discriminator()
discriminator.compile(loss="binary_crossentropy", optimizer=Adam(0.0002, 0.5), metrics=["accuracy"])
discriminator.summary()

def build_gan(generator, discriminator):
    discriminator.trainable = False  # Freeze discriminator
    model = Sequential([
        generator,
        discriminator
    ])
    model.compile(loss="binary_crossentropy", optimizer=Adam(0.0002, 0.5))
    return model

gan = build_gan(generator, discriminator)
gan.summary()

def train_gan(epochs, batch_size):
    real_labels = np.ones((batch_size, 1))
    fake_labels = np.zeros((batch_size, 1))

    for epoch in range(epochs):
        # Get a batch of real images directly from the generator (no need for full dataset in memory)
        real_images = next(train_generator)

        noise = np.random.normal(0, 1, (batch_size, 100))
        fake_images = generator.predict(noise)

        d_loss_real = discriminator.train_on_batch(real_images, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train Generator
        noise = np.random.normal(0, 1, (batch_size, 100))
        g_loss = gan.train_on_batch(noise, real_labels)

        if (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch+1}/{epochs} | D Loss: {d_loss[0]:.4f}, G Loss: {g_loss:.4f}")
            generate_and_save_images(epoch)

train_gan(epochs=500, batch_size=32)

!pip install scikit-image

from skimage.transform import resize
import numpy as np
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.applications.inception_v3 import InceptionV3
from scipy.linalg import sqrtm

# Load the InceptionV3 model (if not already loaded)
inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(75, 75, 3))

def resize_images(images, target_size=(75, 75)):
    """Resizes images to the target size while preserving color channels."""
    resized_images = []
    for img in images:
        resized_img = resize(img, target_size, preserve_range=True) # Resize without removing channels
        resized_images.append(resized_img)
    return np.array(resized_images)  # Convert to numpy array

def calculate_fid(real_images, fake_images):
    """Calculates the Frechet Inception Distance (FID) between real and fake images."""

    # Resize images and preprocess
    real_images = resize_images(real_images, (75, 75))
    fake_images = resize_images(fake_images, (75, 75))

    real_images = preprocess_input(real_images)
    fake_images = preprocess_input(fake_images)

    # Get InceptionV3 activations
    act_real = inception_model.predict(real_images)
    act_fake = inception_model.predict(fake_images)

    # Calculate FID
    mu_real, sigma_real = act_real.mean(axis=0), np.cov(act_real, rowvar=False)
    mu_fake, sigma_fake = act_fake.mean(axis=0), np.cov(act_fake, rowvar=False)

    ssdiff = np.sum((mu_real - mu_fake) ** 2.0)
    covmean = sqrtm(sigma_real.dot(sigma_fake))

    fid = ssdiff + np.trace(sigma_real + sigma_fake - 2.0 * covmean)
    return fid


# Generate fake images for evaluation
num_samples = 100
real_images = next(train_generator)  # Get a batch of real images
noise = np.random.normal(0, 1, (num_samples, 100))
fake_images = generator.predict(noise)

# Compute FID Score
fid_score = calculate_fid(real_images, fake_images)
print(f"FID Score: {fid_score:.2f}")